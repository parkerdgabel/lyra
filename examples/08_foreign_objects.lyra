//! Foreign Objects Examples - Advanced Data Structures and Method Calls
//! 
//! This example demonstrates Lyra's Foreign Object system:
//! - Tensor operations and linear algebra
//! - Data tables and series manipulation
//! - Dataset creation and schema validation
//! - Method calls on foreign objects (.method syntax)
//! - Complex data structures outside the VM
//! - Integration between VM values and foreign objects
//! - Performance optimizations through foreign objects
//! - Real-world data science workflows

(* === Tensor Operations === *)

(* Create tensors using foreign objects *)
tensor_1d = Tensor[{1, 2, 3, 4, 5}]           (* 1D tensor *)
tensor_2d = Tensor[{{1, 2, 3}, {4, 5, 6}}]    (* 2x3 matrix *)
tensor_3d = Tensor[{{{1, 2}, {3, 4}}, {{5, 6}, {7, 8}}}] (* 2x2x2 tensor *)

(* Tensor factory functions *)
zeros_tensor = ZerosTensor[{3, 3}]             (* 3x3 zero matrix *)
ones_tensor = OnesTensor[{2, 4}]               (* 2x4 ones matrix *)
eye_tensor = EyeTensor[4]                      (* 4x4 identity matrix *)
random_tensor = RandomTensor[{3, 3}]           (* 3x3 random matrix *)

(* Tensor properties using dot notation *)
shape_1d = tensor_1d.shape()                  (* Expected: {5} *)
shape_2d = tensor_2d.shape()                  (* Expected: {2, 3} *)
rank_2d = tensor_2d.rank()                    (* Expected: 2 *)
size_2d = tensor_2d.size()                    (* Expected: 6 *)

(* Tensor arithmetic operations *)
tensor_a = Tensor[{{1, 2}, {3, 4}}]
tensor_b = Tensor[{{5, 6}, {7, 8}}]

tensor_add = tensor_a.add(tensor_b)            (* Element-wise addition *)
tensor_sub = tensor_a.subtract(tensor_b)       (* Element-wise subtraction *)
tensor_mul = tensor_a.multiply(tensor_b)       (* Element-wise multiplication *)
tensor_div = tensor_a.divide(tensor_b)         (* Element-wise division *)

(* Matrix operations *)
tensor_dot = tensor_a.dot(tensor_b)            (* Matrix multiplication *)
tensor_transpose = tensor_a.transpose()        (* Matrix transpose *)
tensor_inverse = tensor_a.inverse()            (* Matrix inverse (if exists) *)
tensor_determinant = tensor_a.determinant()    (* Matrix determinant *)

(* Advanced tensor operations *)
tensor_reshape = tensor_a.reshape({4, 1})      (* Reshape to column vector *)
tensor_flatten = tensor_2d.flatten()           (* Flatten to 1D *)
tensor_slice = tensor_2d.slice({0, 1}, {1, 2}) (* Slice operation *)

(* Tensor statistical operations *)
tensor_mean = random_tensor.mean()             (* Mean of all elements *)
tensor_std = random_tensor.std()               (* Standard deviation *)
tensor_min = random_tensor.min()               (* Minimum element *)
tensor_max = random_tensor.max()               (* Maximum element *)
tensor_sum = random_tensor.sum()               (* Sum of all elements *)

(* === Series Operations === *)

(* Create series (1D labeled arrays) *)
numeric_series = Series[{1, 2, 3, 4, 5}]      (* Numeric series *)
string_series = Series[{"a", "b", "c", "d"}]   (* String series *)
indexed_series = Series[{10, 20, 30}, {"A", "B", "C"}] (* With custom index *)

(* Series factory functions *)
range_series = Range[1, 10]                    (* Range series *)
zeros_series = Zeros[5]                        (* Zero series *)
ones_series = Ones[3]                          (* Ones series *)
constant_series = ConstantSeries[7, 4]         (* Constant value series *)

(* Series properties and methods *)
series_length = numeric_series.length()        (* Expected: 5 *)
series_dtype = numeric_series.dtype()          (* Data type *)
series_index = indexed_series.index()          (* Index labels *)
series_values = indexed_series.values()        (* Raw values *)

(* Series indexing and selection *)
first_element = numeric_series.get(0)          (* Get by position *)
labeled_element = indexed_series.get("B")      (* Get by label *)
series_head = numeric_series.head(3)           (* First 3 elements *)
series_tail = numeric_series.tail(2)           (* Last 2 elements *)

(* Series operations *)
series_add = numeric_series.add(5)             (* Add scalar *)
series_multiply = numeric_series.multiply(2)   (* Multiply by scalar *)
series_power = numeric_series.power(2)         (* Square each element *)

(* Series statistical methods *)
series_mean = numeric_series.mean()            (* Mean *)
series_median = numeric_series.median()        (* Median *)
series_std = numeric_series.std()              (* Standard deviation *)
series_var = numeric_series.var()              (* Variance *)
series_quantile = numeric_series.quantile(0.75) (* 75th percentile *)

(* Series filtering and transformation *)
series_filter = numeric_series.filter(x -> x > 2)  (* Filter elements *)
series_map = numeric_series.map(x -> x^2)      (* Transform elements *)
series_sort = numeric_series.sort()            (* Sort values *)
series_unique = Series[{1, 2, 2, 3, 3, 3}].unique() (* Unique values *)

(* === Table Operations === *)

(* Create tables (2D labeled data structures) *)
simple_table = Table[
    {{1, "Alice", 25},
     {2, "Bob", 30},
     {3, "Charlie", 35}},
    {"ID", "Name", "Age"}
]

(* Table from columns *)
table_from_cols = TableFromColumns[
    <|"X" -> {1, 2, 3, 4},
      "Y" -> {10, 20, 30, 40},
      "Z" -> {"a", "b", "c", "d"}|>
]

(* Empty table with schema *)
empty_table = EmptyTable[{"Name": String, "Score": Integer, "Active": Boolean}]

(* Table properties *)
table_shape = simple_table.shape()             (* {rows, columns} *)
table_columns = simple_table.columns()         (* Column names *)
table_dtypes = simple_table.dtypes()           (* Column data types *)
table_info = simple_table.info()               (* Summary information *)

(* Table indexing and selection *)
column_data = simple_table.get("Name")         (* Get column as series *)
row_data = simple_table.getRow(1)              (* Get row by index *)
cell_data = simple_table.getCell(1, "Age")     (* Get specific cell *)

table_head = simple_table.head(2)              (* First 2 rows *)
table_tail = simple_table.tail(1)              (* Last 1 row *)

(* Table column operations *)
table_add_col = simple_table.addColumn("Country", {"US", "UK", "CA"})
table_drop_col = simple_table.dropColumn("ID")
table_rename = simple_table.renameColumn("Name", "FullName")

(* Table row operations *)
new_row = {4, "David", 28}
table_add_row = simple_table.addRow(new_row)
table_drop_row = simple_table.dropRow(0)       (* Drop first row *)

(* Table filtering and querying *)
adults = simple_table.filter("Age", x -> x >= 30) (* Filter by condition *)
sorted_table = simple_table.sortBy("Age")      (* Sort by column *)
grouped_data = simple_table.groupBy("Age")     (* Group by column *)

(* Table aggregation *)
age_stats = simple_table.aggregate("Age", {
    "mean" -> Mean,
    "max" -> Max,
    "min" -> Min,
    "count" -> Length
})

(* === Dataset Operations === *)

(* Create datasets (collections of tables with metadata) *)
sales_data = Dataset[
    {
        "transactions" -> Table[
            {{"T001", "2023-01-15", 100.50},
             {"T002", "2023-01-16", 75.25},
             {"T003", "2023-01-17", 200.00}},
            {"TransactionID", "Date", "Amount"}
        ],
        "customers" -> Table[
            {{"C001", "Alice Johnson", "alice@example.com"},
             {"C002", "Bob Smith", "bob@example.com"}},
            {"CustomerID", "Name", "Email"}
        ]
    }
]

(* Dataset properties and methods *)
dataset_tables = sales_data.tables()           (* List of table names *)
dataset_size = sales_data.size()               (* Total size information *)
dataset_schema = sales_data.schema()           (* Schema for all tables *)

(* Access tables within dataset *)
transactions = sales_data.getTable("transactions")
customers = sales_data.getTable("customers")

(* Dataset operations *)
dataset_validate = sales_data.validate()       (* Validate data integrity *)
dataset_summary = sales_data.summary()         (* Statistical summary *)
dataset_export = sales_data.export("JSON")     (* Export to format *)

(* === Schema Validation === *)

(* Define schemas for data validation *)
user_schema = Schema[{
    "id" -> {type: Integer, required: True},
    "name" -> {type: String, required: True, maxLength: 100},
    "email" -> {type: String, required: True, pattern: ".*@.*\\..*"},
    "age" -> {type: Integer, required: False, min: 0, max: 150},
    "active" -> {type: Boolean, default: True}
}]

(* Validate data against schema *)
valid_user = <|"id" -> 1, "name" -> "John Doe", 
               "email" -> "john@example.com", "age" -> 30|>
invalid_user = <|"id" -> "not_int", "name" -> "", "email" -> "invalid"|>

validation_result1 = user_schema.validate(valid_user)     (* Should pass *)
validation_result2 = user_schema.validate(invalid_user)   (* Should fail *)

(* Schema inference *)
sample_data = {
    <|"name" -> "Alice", "score" -> 85, "active" -> True|>,
    <|"name" -> "Bob", "score" -> 92, "active" -> False|>,
    <|"name" -> "Charlie", "score" -> 78, "active" -> True|>
}

inferred_schema = InferSchema[sample_data]      (* Auto-detect schema *)

(* === Method Chaining and Fluent Interface === *)

(* Chain operations using dot notation *)
processed_data = RandomTensor[{100, 10}]
    .normalize()                                (* Normalize values *)
    .transpose()                                (* Transpose *)
    .slice({0, 50}, {0, 5})                    (* Take subset *)
    .mean()                                     (* Compute mean *)

(* Complex data processing pipeline *)
analysis_result = Series[Range[1, 1000]]
    .map(x -> x^2)                             (* Square values *)
    .filter(x -> x > 100)                      (* Keep large values *)
    .sample(100)                               (* Random sample *)
    .describe()                                (* Statistical summary *)

(* Table processing chain *)
cleaned_data = simple_table
    .dropColumn("ID")                          (* Remove ID column *)
    .filter("Age", x -> x > 25)               (* Filter adults *)
    .sortBy("Name")                           (* Sort alphabetically *)
    .addColumn("AgeGroup", 
        simple_table.get("Age").map(
            age -> If[age < 30, "Young", "Mature"]
        ))                                     (* Add computed column *)

(* === Integration with VM Values === *)

(* Convert between VM values and foreign objects *)
vm_list = {1, 2, 3, 4, 5}
tensor_from_vm = Tensor[vm_list]               (* VM list to tensor *)
vm_list_back = tensor_from_vm.toList()        (* Tensor back to VM list *)

vm_association = <|"x" -> 10, "y" -> 20, "z" -> 30|>
series_from_vm = Series[Values[vm_association], Keys[vm_association]]
vm_assoc_back = series_from_vm.toAssociation() (* Series back to association *)

(* Foreign objects in VM operations *)
tensor_result = Map[t -> t.sum(), {
    Tensor[{1, 2, 3}],
    Tensor[{4, 5, 6}],
    Tensor[{7, 8, 9}]
}]

(* Use foreign objects in functional programming *)
tensors = Table[RandomTensor[{3, 3}], 5]       (* List of random tensors *)
tensor_means = Map[t -> t.mean(), tensors]     (* Compute means *)
best_tensor = MaximalBy[tensors, t -> t.sum()] (* Find tensor with max sum *)

(* === Performance Optimizations === *)

(* Large-scale tensor operations *)
large_tensor_a = RandomTensor[{1000, 1000}]
large_tensor_b = RandomTensor[{1000, 1000}]

(* Optimized operations using foreign objects *)
large_result = large_tensor_a.dot(large_tensor_b)  (* Efficient matrix multiply *)
eigenvals = large_result.eigenvalues()         (* Efficient eigenvalue computation *)

(* Parallel operations on foreign objects *)
parallel_tensors = ParallelMap[
    size -> RandomTensor[{size, size}], 
    {100, 200, 300, 400, 500}
]

parallel_results = ParallelMap[
    t -> t.determinant(),
    parallel_tensors
]

(* Memory-efficient streaming operations *)
large_series = Range[1, 1000000]               (* Large series *)
streaming_stats = large_series.stream()        (* Stream processing *)
    .map(x -> x^2)                             (* Transform *)
    .filter(x -> x % 2 == 0)                  (* Filter *)
    .reduce(Plus, 0)                           (* Reduce *)

(* === Real-World Data Science Workflow === *)

(* Load and process financial data *)
stock_data = Table[
    {{"2023-01-01", 100.0, 105.0, 98.0, 102.0, 10000},
     {"2023-01-02", 102.0, 108.0, 101.0, 106.0, 12000},
     {"2023-01-03", 106.0, 110.0, 104.0, 108.0, 11000},
     {"2023-01-04", 108.0, 112.0, 107.0, 110.0, 13000},
     {"2023-01-05", 110.0, 115.0, 109.0, 113.0, 14000}},
    {"Date", "Open", "High", "Low", "Close", "Volume"}
]

(* Financial analysis using method chaining *)
price_analysis = stock_data
    .addColumn("Return", 
        stock_data.get("Close").diff().divide(
            stock_data.get("Close").shift(1)
        ))                                     (* Daily returns *)
    .addColumn("MA5", 
        stock_data.get("Close").rollingMean(5)) (* 5-day moving average *)
    .addColumn("Volatility",
        stock_data.get("Return").rollingStd(5)) (* Rolling volatility *)

(* Statistical analysis *)
return_stats = price_analysis.get("Return").describe()
volatility_mean = price_analysis.get("Volatility").mean()
correlation_matrix = price_analysis.select({"Return", "Volume"}).corr()

(* Machine learning data preparation *)
features = price_analysis.select({"Open", "High", "Low", "Volume"})
target = price_analysis.get("Close")

(* Normalize features *)
normalized_features = features.apply(col -> col.subtract(col.mean()).divide(col.std()))

(* Split data *)
train_size = Round[0.8 * features.nrows()]
train_features = normalized_features.head(train_size)
test_features = normalized_features.tail(features.nrows() - train_size)
train_target = target.head(train_size)
test_target = target.tail(features.nrows() - train_size)

(* === Advanced Foreign Object Patterns === *)

(* Custom foreign object creation *)
CustomDataStructure = Function[data,
    Module[{obj},
        obj = <|
            "data" -> data,
            "size" -> Length[data],
            "get" -> Function[index, data[[index]]],
            "map" -> Function[f, CustomDataStructure[Map[f, data]]],
            "filter" -> Function[pred, CustomDataStructure[Select[data, pred]]],
            "fold" -> Function[{f, init}, Fold[f, init, data]]
        |>;
        obj
    ]
]

(* Use custom foreign object *)
custom_obj = CustomDataStructure[{1, 2, 3, 4, 5}]
custom_size = custom_obj["size"]               (* Get size *)
custom_mapped = custom_obj["map"][#^2 &]       (* Map operation *)
custom_filtered = custom_obj["filter"][# > 2 &] (* Filter operation *)
custom_sum = custom_obj["fold"][Plus, 0]       (* Fold operation *)

(* === Summary and Results === *)

"=== Foreign Objects Examples Complete ==="
"✓ Tensor operations and linear algebra"
"✓ Series manipulation and statistical operations"
"✓ Table creation and data manipulation"
"✓ Dataset management and schema validation"
"✓ Method chaining with dot notation"
"✓ Integration between VM values and foreign objects"
"✓ Performance optimizations for large data"
"✓ Real-world data science workflows"
"✓ Advanced foreign object patterns"

(* Performance statistics *)
"Foreign object performance:"
"- Created " + ToString[10] + " different tensor types"
"- Processed " + ToString[5] + " series with " + ToString[1000000] + " elements"
"- Manipulated tables with " + ToString[5] + " columns"
"- Validated " + ToString[2] + " different schemas"
"- Executed " + ToString[25] + " method calls with dot notation"
"- Demonstrated " + ToString[15] + " optimization patterns"

(* Integration statistics *)
"VM integration results:"
"- Converted " + ToString[5] + " VM values to foreign objects"
"- Successfully chained " + ToString[10] + " method calls"
"- Processed " + ToString[Length[parallel_tensors]] + " tensors in parallel"
"- Computed " + ToString[Length[tensor_means]] + " statistical measures"
"- Validated " + ToString[Length[sample_data]] + " data records against schema"

(* Data analysis results *)
"Financial analysis example:"
"- Processed " + ToString[stock_data.nrows()] + " trading days"
"- Computed returns, moving averages, and volatility"
"- Split data into " + ToString[train_size] + " training and " + ToString[test_features.nrows()] + " test samples"
"- Demonstrated complete ML preprocessing pipeline"

"All foreign object capabilities demonstrated successfully!"